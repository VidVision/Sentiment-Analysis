{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Topic Coherence\n",
    "\n",
    "The purpose of this notebook is to validate the LDA model using topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Setting Variables\n",
    "\n",
    "Change the variables below in order to change how the notebook functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False #Set to True if testing the script. Set to False if running the script fully.\n",
    "lda_file = \"data/ldaoutput.csv.gz\" #The path to the output generated by the Apply VADER Model notebook.\n",
    "lda_model = \"final.lda\" #Path to the lda model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiating lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating translation table to filter out punctuation\n",
    "#https://stackoverflow.com/questions/11692199/string-translate-with-unicode-data-in-python\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating gensim dict\n",
    "#https://stackoverflow.com/questions/62602646/removing-the-non-english-data\n",
    "#https://stackoverflow.com/questions/39634222/is-there-a-way-to-remove-proper-nouns-from-a-sentence-using-python/39635503\n",
    "tagged_set = pos_tag(words.words(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235886"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set([word.lower() for word, tag in tagged_set if tag != \"NNP\" and tag != \"NNPS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set.add(\"covid\")\n",
    "word_set.add(\"corona\")\n",
    "word_set.add(\"coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimdict = gensim.corpora.Dictionary([word_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating master list of stop words\n",
    "master_stop = [\"would\", \"also\", \"even\", \"u\", \"one\", \"could\"] + stopwords.words(\"english\")\n",
    "master_stop = set(master_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to clean docs\n",
    "def clean(doc):\n",
    "    normalized = []\n",
    "    try:\n",
    "        doc = doc.translate(translate_table).lower()\n",
    "        doc_list = doc.strip().split()\n",
    "        for word in doc_list:\n",
    "            word = word.strip()\n",
    "            lemma = wnl.lemmatize(word)\n",
    "            if len(lemma)>2 and lemma in word_set and lemma not in master_stop:\n",
    "                normalized.append(lemma)\n",
    "    except AttributeError: #In case of error in cleaning doc\n",
    "        pass\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Model Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherences(model_path, dic, lda_path):\n",
    "    if testing is True:\n",
    "        size = 5\n",
    "        train_size = 0.3\n",
    "        chunksize = 100000\n",
    "    else:\n",
    "        size = 40\n",
    "        train_size = 0.025\n",
    "        chunksize = 250000\n",
    "    coherences = []\n",
    "    sample_sizes = []\n",
    "    np.random.seed(8283)\n",
    "    seeds = np.random.randint(low = 1, high=999999, size = size)\n",
    "    model = gensim.models.LdaModel.load(model_path)\n",
    "    for index, seed in enumerate(seeds):\n",
    "        print(\"Processing seed %s of %s\" % (index+1, size))\n",
    "        chunks = pd.read_csv(lda_path, compression = \"gzip\", chunksize = chunksize, usecols = ['comments', 'best topic'])\n",
    "        mini_chunks = []\n",
    "        for chunk in chunks:\n",
    "            chunk.dropna(inplace = True)\n",
    "            sample, dump = train_test_split(chunk, train_size = train_size, random_state = seed, stratify = chunk['best topic'])\n",
    "            del dump\n",
    "            mini_chunks += [gensimdict.doc2bow(clean(doc)) for doc in sample[\"comments\"].to_list()]\n",
    "        sample_sizes.append(len(mini_chunks))\n",
    "        #https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "        cm = gensim.models.coherencemodel.CoherenceModel(model = model, corpus = mini_chunks, dictionary = dic, coherence = \"u_mass\")\n",
    "        coherences.append(cm.get_coherence())\n",
    "    return coherences, sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing seed 1 of 40\n",
      "Processing seed 2 of 40\n",
      "Processing seed 3 of 40\n",
      "Processing seed 4 of 40\n",
      "Processing seed 5 of 40\n",
      "Processing seed 6 of 40\n",
      "Processing seed 7 of 40\n",
      "Processing seed 8 of 40\n",
      "Processing seed 9 of 40\n",
      "Processing seed 10 of 40\n",
      "Processing seed 11 of 40\n",
      "Processing seed 12 of 40\n",
      "Processing seed 13 of 40\n",
      "Processing seed 14 of 40\n",
      "Processing seed 15 of 40\n",
      "Processing seed 16 of 40\n",
      "Processing seed 17 of 40\n",
      "Processing seed 18 of 40\n",
      "Processing seed 19 of 40\n",
      "Processing seed 20 of 40\n",
      "Processing seed 21 of 40\n",
      "Processing seed 22 of 40\n",
      "Processing seed 23 of 40\n",
      "Processing seed 24 of 40\n",
      "Processing seed 25 of 40\n",
      "Processing seed 26 of 40\n",
      "Processing seed 27 of 40\n",
      "Processing seed 28 of 40\n",
      "Processing seed 29 of 40\n",
      "Processing seed 30 of 40\n",
      "Processing seed 31 of 40\n",
      "Processing seed 32 of 40\n",
      "Processing seed 33 of 40\n",
      "Processing seed 34 of 40\n",
      "Processing seed 35 of 40\n",
      "Processing seed 36 of 40\n",
      "Processing seed 37 of 40\n",
      "Processing seed 38 of 40\n",
      "Processing seed 39 of 40\n",
      "Processing seed 40 of 40\n"
     ]
    }
   ],
   "source": [
    "coherences, sample_sizes = get_coherences(lda_model, gensimdict, lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.1154050880184756,\n",
       " -3.102062077021193,\n",
       " -3.1101989039722278,\n",
       " -3.126496501857887,\n",
       " -3.1150928401828235,\n",
       " -3.1262551378979095,\n",
       " -3.0877197918933232,\n",
       " -3.121917245466807,\n",
       " -3.0851024029663923,\n",
       " -3.0889992058043387,\n",
       " -3.0882556633270775,\n",
       " -3.1504983219830454,\n",
       " -3.1014787493018146,\n",
       " -3.1459057761632456,\n",
       " -3.138697578483115,\n",
       " -3.1451226536393433,\n",
       " -3.113505082479159,\n",
       " -3.1542921403681476,\n",
       " -3.1144359814136293,\n",
       " -3.1042901945308805,\n",
       " -3.056579616588082,\n",
       " -3.107275824284513,\n",
       " -3.1560037787710784,\n",
       " -3.1118898120826723,\n",
       " -3.129518230658376,\n",
       " -3.1178083120944846,\n",
       " -3.0927170067512897,\n",
       " -3.1068986310191375,\n",
       " -3.1650571301025825,\n",
       " -3.1198842901116963,\n",
       " -3.0728145067106687,\n",
       " -3.105452911675316,\n",
       " -3.1359747107644393,\n",
       " -3.122473751836836,\n",
       " -3.1249414511922184,\n",
       " -3.100564835261313,\n",
       " -3.0868108979833266,\n",
       " -3.0902831460723252,\n",
       " -3.105505910605413,\n",
       " -3.133365817624767]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1650571301025825, -3.114438797724034, -3.056579616588082)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(coherences), np.mean(coherences), np.max(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782,\n",
       " 254782]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10191280"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sample_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
